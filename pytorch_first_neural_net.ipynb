{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: A simple two layer fully connected network\n",
    "\n",
    "Here we will go step by step through implmenting a very simple 2 layer fully connected network to label MNIST digits.\n",
    "\n",
    "We start with a number of extra imports: pyplot lets us create plots, torch.nn is a torch library for implementing neural nets, and transforms is a torchvision package to help us work with image data, and datasets is a torchvision package granting us access to some built in test data. We will look at importing data in a later example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "training_data = dsets.MNIST(root=\"./data\", train = True, transform=transforms.ToTensor(), download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a **validation set** of data as well, but the built in MNIST caller does not support a validation set, and we will just use the testing data set as the validation set for purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = dsets.MNIST(root=\"./data\", train = False, transform=transforms.ToTensor(), download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f0d7b1450>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f12097a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The mnist data comes in as a tuple. The first index is the pixel values packed in a pytorch Tensor, \n",
    "## the 2nd index is the class label.\n",
    "# To show the image I have to make pytorch tensor a \n",
    "training_image_tensor = training_data[890][0]\n",
    "\n",
    "# Note that the image is a 1x28x28 tensor: there is only one color channel; it is grayscale. \n",
    "print(training_image_tensor.shape)\n",
    "plt.imshow(training_image_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "class_label = training_data[890][1]\n",
    "print(class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch comes with **built-in DataLoaders** that make it trivial to create minibatches and to enumerate over them. Always use these helpers!!\n",
    "\n",
    "A DataLoader takes as input the desired minibatch size you would like (batch_size), and whether or not you would like the data to be shuffled before it is partitioned into minibatches. No shuffling means the exact same minibatches will be presented to your network every epoch. There is not a lot of guidance as to whether shuffling is good or bad, but intuition says shuffling should help with generalizability a bit.\n",
    "\n",
    "The DataLoader function returns an *iterator* over your datasets with shuffling and batchsize specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data, batch_size=len(testing_data), shuffle=False)\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a deep net\n",
    "### 1. Specify and instantiate the network.\n",
    "\n",
    "Here we go. A DNN is specified as a class that inherets from nn.Module. They all have the same structure:\n",
    "<ol>\n",
    "    <li> a call to the constructor of nn.Module (super)\n",
    "    <li> specification of a number of network layers. It is common practice to wrap the way the nodes are wired and the activation function (and any other processing, like doing batch norm) inside of an nn.Sequential object. nn.Sequential is just a convinent wrapper of a number of nn.function calls. \n",
    "    <li> Definition of a **forward** function. This function must be specified. This function describes the way that data will pass through your deep net. Here, we see that the input will bass through fc1, then fc2, and we return thr output of fc2. \n",
    "        </ol>\n",
    "        \n",
    "A few things to note: \n",
    "<ol> \n",
    "    <li> We do not need to tell the deep net what the size of the minibatch we will present to it is.\n",
    "    <li> nn.Linear specifies a linear combination of inputs into a layer. The linear combination is the $\\sum_i w_i x_i$ we are used to seeing, where the summation is over **every node in the previous layer**. So nn.Linear specifies a fully-connected connectivity in this layer. \n",
    "<li> We simply output the ReLU activation values of fc2 -- we do not normalize these values so they sum to 1. We may want to do a normalization here if we want the network to output a score for the likelihood the input is of each class that looks like a probability. We could do so by specifiying a **softmax** layer. But this is equivalent to just predicting the class corresponding to the fc2 output that is maximum.\n",
    "    </ol>\n",
    "    \n",
    "\n",
    "**The PyTorch docs are a great place to see what types of layers and activations are implemented**. Of course, nothing is stoping you from implementing your own layer or activation function and then call those functions inside of the deep net specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(DeepNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=10)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "[Parameter containing:\n",
      "1.00000e-02 *\n",
      " 2.5436 -2.1927 -3.3524  ...   3.0668  1.8614  3.2343\n",
      " 1.4068 -1.3977 -2.5394  ...  -0.7758 -1.9729  2.4468\n",
      "-3.2340  0.5947 -3.3617  ...  -3.5204  1.7534 -2.2791\n",
      "          ...             ⋱             ...          \n",
      " 0.1256  2.1510 -2.2834  ...   2.0234 -1.0439 -3.3102\n",
      "-2.8738  2.7599  2.5549  ...  -3.0116 -3.1609 -0.8644\n",
      "-0.6734  1.5868  3.0672  ...  -1.1066 -2.4457  3.2477\n",
      "[torch.FloatTensor of size 10x784]\n",
      ", Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.4262\n",
      " -2.0045\n",
      "  0.7448\n",
      "  2.1001\n",
      " -0.3384\n",
      " -2.2033\n",
      "  2.4331\n",
      "  3.0071\n",
      "  3.4535\n",
      " -1.3281\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      " 0.1248 -0.3092  0.1243 -0.1529 -0.0489  0.2485 -0.2061 -0.0625 -0.0925 -0.1836\n",
      " 0.1565 -0.2618  0.2047 -0.2585  0.2483  0.0656  0.1975  0.2855  0.1935  0.1130\n",
      "-0.3114  0.2089 -0.2109 -0.2287 -0.3162 -0.0833  0.2638 -0.1494  0.1019 -0.2025\n",
      " 0.2662  0.1302 -0.1666 -0.1696  0.0224  0.1577  0.2719  0.0282 -0.1737  0.0523\n",
      " 0.2110  0.1336  0.2392  0.1208 -0.3100  0.1550  0.1696 -0.2218  0.0313 -0.0253\n",
      "-0.0660  0.0938 -0.1160 -0.0767  0.0857 -0.2709 -0.0667  0.0889 -0.2848 -0.2883\n",
      " 0.1117  0.2866  0.2904 -0.3020 -0.1503  0.1833 -0.1875  0.2319 -0.1154 -0.1431\n",
      " 0.2684  0.1418 -0.2807  0.1213  0.3115 -0.1391  0.2284 -0.0425 -0.1391  0.0232\n",
      "-0.2282  0.0132 -0.2468  0.2521 -0.1795 -0.0424 -0.0556  0.1260  0.3044  0.0721\n",
      "-0.0943 -0.2553  0.1955 -0.1125  0.2486  0.0297 -0.1059  0.0291  0.2666 -0.2971\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      "-0.0510\n",
      "-0.2370\n",
      " 0.1620\n",
      " 0.0735\n",
      " 0.0409\n",
      "-0.2272\n",
      "-0.2771\n",
      "-0.2239\n",
      "-0.2650\n",
      "-0.1283\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      "-0.3098  0.0528 -0.2161 -0.1235  0.1357  0.1983 -0.1684 -0.1048  0.2910  0.2453\n",
      " 0.1971 -0.2044  0.2669 -0.3131 -0.2939 -0.2003  0.1409 -0.1482  0.1366  0.1086\n",
      " 0.1238  0.0882  0.1126  0.0775  0.1893 -0.1466 -0.0006  0.0489 -0.2574  0.1065\n",
      " 0.1761 -0.1618 -0.1239  0.1487 -0.1771  0.1330 -0.2807  0.0497  0.1227  0.2485\n",
      " 0.1292  0.3115 -0.0424 -0.2841  0.0460 -0.0260 -0.2770 -0.2284  0.2982 -0.1160\n",
      " 0.1198 -0.3064  0.1104 -0.0549 -0.1064 -0.0214  0.1088  0.1654 -0.1460 -0.0111\n",
      " 0.0872  0.2049 -0.0353 -0.0393  0.2356 -0.0135  0.2160  0.0137  0.1392 -0.0915\n",
      "-0.1973  0.1910  0.1073 -0.2114  0.1294 -0.0394 -0.0613  0.0664  0.2545  0.0821\n",
      " 0.0812 -0.3069 -0.1981 -0.0541  0.0743 -0.1999 -0.0555 -0.2673  0.0153  0.1432\n",
      " 0.0244 -0.2518  0.0122  0.2364  0.2464 -0.2766 -0.0653  0.2933 -0.0558  0.0722\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      " 0.2450\n",
      " 0.1077\n",
      "-0.0482\n",
      " 0.2755\n",
      "-0.0746\n",
      "-0.0784\n",
      "-0.0382\n",
      "-0.0976\n",
      " 0.1022\n",
      " 0.0547\n",
      "[torch.FloatTensor of size 10]\n",
      "]\n",
      "[Parameter containing:\n",
      "1.00000e-02 *\n",
      " 2.5436 -2.1927 -3.3524  ...   3.0668  1.8614  3.2343\n",
      " 1.4068 -1.3977 -2.5394  ...  -0.7758 -1.9729  2.4468\n",
      "-3.2340  0.5947 -3.3617  ...  -3.5204  1.7534 -2.2791\n",
      "          ...             ⋱             ...          \n",
      " 0.1256  2.1510 -2.2834  ...   2.0234 -1.0439 -3.3102\n",
      "-2.8738  2.7599  2.5549  ...  -3.0116 -3.1609 -0.8644\n",
      "-0.6734  1.5868  3.0672  ...  -1.1066 -2.4457  3.2477\n",
      "[torch.FloatTensor of size 10x784]\n",
      ", Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.4262\n",
      " -2.0045\n",
      "  0.7448\n",
      "  2.1001\n",
      " -0.3384\n",
      " -2.2033\n",
      "  2.4331\n",
      "  3.0071\n",
      "  3.4535\n",
      " -1.3281\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      " 10.0177   9.9859   9.9952   9.9986   9.9984   9.9952  10.0007  10.0144\n",
      " 10.0093   9.9888   9.9861   9.9974   9.9964   9.9848  10.0168   9.9930\n",
      " 10.0008  10.0026  10.0060  10.0119  10.0049   9.9875  10.0130  10.0182\n",
      " 10.0040   9.9879  10.0225  10.0017   9.9973  10.0063  10.0134   9.9993\n",
      "  9.9924  10.0068   9.9972   9.9852  10.0015  10.0049   9.9842  10.0162\n",
      "  9.9985  10.0049  10.0134  10.0022   9.9930  10.0053   9.9859  10.0104\n",
      "  9.9925  10.0180  10.0034   9.9995  10.0031  10.0022  10.0084  10.0097\n",
      "  9.9964   9.9921   9.9927  10.0105  10.0151  10.0194  10.0124   9.9885\n",
      "  9.9962   9.9986  10.0067  10.0234  10.0122  10.0011   9.9988  10.0108\n",
      "  9.9958   9.9828  10.0045  10.0065   9.9946   9.9997   9.9880  10.0076\n",
      "\n",
      "Columns 8 to 9 \n",
      "  9.9953  10.0086\n",
      "  9.9851   9.9941\n",
      "  9.9922  10.0002\n",
      "  9.9998   9.9961\n",
      " 10.0068   9.9918\n",
      " 10.0274  10.0212\n",
      " 10.0019   9.9966\n",
      " 10.0012   9.9948\n",
      "  9.9885   9.9964\n",
      " 10.0130   9.9978\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      " 50.0038\n",
      " 49.9996\n",
      " 49.9895\n",
      " 49.9964\n",
      " 49.9927\n",
      " 49.9999\n",
      " 50.0074\n",
      " 50.0017\n",
      " 49.9951\n",
      " 50.0015\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      "-0.3098  0.0528 -0.2161 -0.1235  0.1357  0.1983 -0.1684 -0.1048  0.2910  0.2453\n",
      " 0.1971 -0.2044  0.2669 -0.3131 -0.2939 -0.2003  0.1409 -0.1482  0.1366  0.1086\n",
      " 0.1238  0.0882  0.1126  0.0775  0.1893 -0.1466 -0.0006  0.0489 -0.2574  0.1065\n",
      " 0.1761 -0.1618 -0.1239  0.1487 -0.1771  0.1330 -0.2807  0.0497  0.1227  0.2485\n",
      " 0.1292  0.3115 -0.0424 -0.2841  0.0460 -0.0260 -0.2770 -0.2284  0.2982 -0.1160\n",
      " 0.1198 -0.3064  0.1104 -0.0549 -0.1064 -0.0214  0.1088  0.1654 -0.1460 -0.0111\n",
      " 0.0872  0.2049 -0.0353 -0.0393  0.2356 -0.0135  0.2160  0.0137  0.1392 -0.0915\n",
      "-0.1973  0.1910  0.1073 -0.2114  0.1294 -0.0394 -0.0613  0.0664  0.2545  0.0821\n",
      " 0.0812 -0.3069 -0.1981 -0.0541  0.0743 -0.1999 -0.0555 -0.2673  0.0153  0.1432\n",
      " 0.0244 -0.2518  0.0122  0.2364  0.2464 -0.2766 -0.0653  0.2933 -0.0558  0.0722\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      " 0.2450\n",
      " 0.1077\n",
      "-0.0482\n",
      " 0.2755\n",
      "-0.0746\n",
      "-0.0784\n",
      "-0.0382\n",
      "-0.0976\n",
      " 0.1022\n",
      " 0.0547\n",
      "[torch.FloatTensor of size 10]\n",
      "]\n",
      "[Parameter containing:\n",
      "-6.9333e-03 -3.8987e-03  4.3111e-03  ...   1.0071e-02 -3.2693e-03 -2.1465e-02\n",
      "-1.3219e-02  1.3584e-02 -3.4353e-02  ...   4.3695e-03  2.1827e-03  1.8436e-02\n",
      "-3.2045e-02  1.5981e-02 -3.1610e-02  ...  -1.8946e-02  1.5791e-02  2.6620e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 1.2730e-02 -2.0597e-02  4.5482e-03  ...   3.2265e-02 -1.0771e-02 -7.2060e-03\n",
      " 1.3142e-02  1.4454e-02 -1.6190e-02  ...   1.0851e-02  1.3865e-03 -1.1458e-02\n",
      "-2.4839e-02  2.5785e-02 -1.1370e-02  ...   2.3801e-02 -3.0834e-02  3.5177e-02\n",
      "[torch.FloatTensor of size 10x784]\n",
      ", Parameter containing:\n",
      "1.00000e-02 *\n",
      "  1.4460\n",
      "  3.1542\n",
      "  2.6421\n",
      " -1.9068\n",
      "  2.7696\n",
      "  0.6319\n",
      "  3.0326\n",
      " -0.5811\n",
      "  0.8624\n",
      "  3.2418\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      " 0.2101  0.1695  0.2275 -0.0253  0.1524  0.0026  0.0646  0.1381 -0.2761 -0.1742\n",
      "-0.0330  0.1651 -0.1331  0.1331  0.2072 -0.0381 -0.0499  0.0130  0.2147  0.1801\n",
      " 0.2222  0.2846  0.2039  0.1831 -0.1103 -0.1720  0.2538 -0.1687 -0.1368 -0.2709\n",
      " 0.0102 -0.1999  0.1912 -0.2216  0.0117  0.0082  0.0577 -0.2074 -0.3076  0.1835\n",
      "-0.0078 -0.1391  0.2952  0.0229 -0.2497  0.2612 -0.1164  0.2577  0.0695 -0.2388\n",
      " 0.2211  0.0881 -0.0947 -0.0151 -0.0718 -0.1679 -0.1770 -0.0618 -0.0074  0.0174\n",
      " 0.0256  0.0893 -0.2919 -0.2228  0.2266  0.3125  0.1938  0.2491 -0.1764 -0.0773\n",
      " 0.2031  0.0687 -0.1020 -0.0792  0.2451 -0.0276  0.3141  0.0593  0.0980 -0.1899\n",
      "-0.2213  0.1857  0.0256  0.2974 -0.1914 -0.1599  0.0821 -0.3004 -0.2510  0.1402\n",
      " 0.1570 -0.0195 -0.2037  0.2726  0.0555  0.1830 -0.2468  0.2951  0.2059 -0.1868\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      "-0.0582\n",
      "-0.1969\n",
      "-0.1988\n",
      "-0.1186\n",
      " 0.0907\n",
      " 0.0589\n",
      "-0.1226\n",
      "-0.2509\n",
      "-0.2934\n",
      " 0.2763\n",
      "[torch.FloatTensor of size 10]\n",
      ", Parameter containing:\n",
      " 0.2201 -0.3040  0.1057  0.1714 -0.1173 -0.2448 -0.1392 -0.2948 -0.0701 -0.2227\n",
      "-0.0376  0.1586 -0.2344  0.1002 -0.1869 -0.0661  0.2398  0.0689 -0.2153 -0.2089\n",
      " 0.0771  0.1994  0.1201 -0.2627 -0.2660 -0.1412 -0.1539  0.2172 -0.0027  0.2541\n",
      "-0.2407 -0.2368  0.0794 -0.1576 -0.2460 -0.1468  0.2780 -0.0615 -0.1110 -0.1358\n",
      "-0.0693 -0.2514  0.1497 -0.0169 -0.0520 -0.0684  0.2542  0.2733 -0.0310  0.1410\n",
      "-0.0824  0.0704  0.1974 -0.1649  0.1729  0.2121  0.2073 -0.0821 -0.0142 -0.0331\n",
      " 0.0826 -0.0796  0.1292 -0.1252  0.0061  0.0662 -0.2772 -0.2863  0.0007 -0.0473\n",
      " 0.1059  0.2644  0.3028  0.0770 -0.1034 -0.1977  0.0354 -0.0113 -0.2054  0.0141\n",
      " 0.1053 -0.1466 -0.1095  0.2880  0.2680 -0.2269  0.1650  0.2203  0.1314 -0.2699\n",
      "-0.2245  0.1325  0.2651  0.1333  0.2445  0.0482 -0.2117 -0.2592  0.1591  0.2451\n",
      "[torch.FloatTensor of size 10x10]\n",
      ", Parameter containing:\n",
      " 0.2438\n",
      "-0.2046\n",
      " 0.0915\n",
      " 0.0317\n",
      " 0.3120\n",
      " 0.1940\n",
      " 0.0717\n",
      "-0.1955\n",
      " 0.0705\n",
      "-0.0977\n",
      "[torch.FloatTensor of size 10]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "input_size = 28*28 ## Note that an MNIST image is 28*28 pixels.\n",
    "num_classes = 10\n",
    "hidden_size = 10\n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(the_net)\n",
    "\n",
    "## the parameters() function of an nn.Module object returns an iterator over \n",
    "## all of the network parameters (e.g. weights.) PyTorch initializes all weights in linear and\n",
    "## conv layers as: \n",
    "## stdv = 1. / math.sqrt(number_of_layer_inputs)\n",
    "## self.weight.data.uniform_(-stdv, stdv)\n",
    "## So with uniformly distributed weights within -stdv, stdv.\n",
    "\n",
    "## If you want to see what the parameters are, iterate over parameters() or make it a list:\n",
    "params = list(the_net.parameters())\n",
    "## We print the parameters of the network in order of weights, then biases, for each layer that has weights and \n",
    "## biases, in sequential order of operation in the forward pass. \n",
    "print(params)\n",
    "\n",
    "## If you want to specify your own weight initiaization routine, you can do it simply!\n",
    "## It is possible to . through the structure of the_net. any Sequential layers are lists, so fc2[0] is the \n",
    "## nn.Linear in fc2. fc2[1] is the ReLU activation.\n",
    "## Remember that _ suffix means in place updating.\n",
    "the_net.fc2[0].weight.data.normal_(10,0.01)\n",
    "the_net.fc2[0].bias.data.normal_(50,0.01)\n",
    "print(params)\n",
    "\n",
    "## anyway, these weights are horrible. Let's reinitialize the network. :) \n",
    "the_net = DeepNet(input_size, hidden_size, num_classes)\n",
    "print(list(the_net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define your loss function.\n",
    "\n",
    "Since we are tackling a classification problem we want to use cross entropy loss. For those who know cross entropy (or have heard of **entropy** before) would know that the loss must operate on a probability distribution. And indeed it does: this loss expects the probability the network thinks the input is of each class. In PyTorch, **this normalization (e.g. passing the output of fc2 through a *softmax layer*) is done automatically by the loss function**. I (Derek) don't really like this, its a bit of a gotcha! Now you know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define your optimizer.\n",
    "\n",
    "When in doubt, start with ADAM since it adapts its learning rate over time. Start with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(the_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training.\n",
    "We will train in a loop that iterates over the minibatches yielded from train_loader, for a fixed number of epochs. Here is what's going on: \n",
    "<ol>\n",
    "    <li> We are specifying a loop for 10 epochs. In the loop we wrap the train_loader in an enumerator. The enumerator will yield an enumeration count and a tuple (minibatch_of_images, minibatch_of_labels). \n",
    "    <li> The minibatch_of_images is a 4D tensor with dimensions (size_of_minibatch, depth, width, height). The network expects as input a vector of size 28*28 so we need to reshape the tensor accordingly. We will reshape it to a matrix of 32 rows, 28*28 columns. Note that we can pass -1 as the first parameter of view, this is like saying \"I don't know how many rows I'll need, just make as many as needed to the column count I specify (28*28) is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 784])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for _ , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "        print minibatch_of_images.shape\n",
    "        print minibatch_of_images.view(32, 28*28).shape\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> We build a computation graph here to setup backprop! So wrap the labels and the batch in a Variable.\n",
    "    <li> Tell the optimizer to zero out any gradient stored in the Variables of the parameters of the_net. Indeed, the optimizer knows of the_net's parameters as we passed them in as a required parameter of the optimizer above. **Zero the gradient out after every minibatch!**\n",
    "     <li> Okay, run the_batch through the_net. This is the **forward pass**. Note that outputs will be a Variable.\n",
    "     <li> Compute the loss, how much error the network had on the_batch.\n",
    "     <li> Now backprop! loss is a real number value, so that will be the gradient signal. Observe. loss is a functino of the outputs, which is a function of the_net, which is a function of the_batch. So the computation chain is all setup! \n",
    "      <li> Tell the optimizer to update the weights by calling step(). Step tells the optimizer to use the gradients stored at each network parameter (computed by calling backward() on loss) to compute and apply a weight update.\n",
    "      <li> Give some console output to see how we are doing every few minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, minibatch 0. Loss: 0.6616. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7398\n",
      "At epoch 0, minibatch 600. Loss: 0.6977. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7418\n",
      "At epoch 0, minibatch 1200. Loss: 0.9206. Accuracy on the batch: 0.6250. Accuracy on the test data: 0.7447\n",
      "At epoch 0, minibatch 1800. Loss: 0.6930. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7457\n",
      "Epoch 0 finished! It took: 6.5924 seconds\n",
      "At epoch 1, minibatch 0. Loss: 0.6969. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7444\n",
      "At epoch 1, minibatch 600. Loss: 0.6678. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7465\n",
      "At epoch 1, minibatch 1200. Loss: 0.6927. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7489\n",
      "At epoch 1, minibatch 1800. Loss: 0.6514. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7482\n",
      "Epoch 1 finished! It took: 6.6015 seconds\n",
      "At epoch 2, minibatch 0. Loss: 0.8018. Accuracy on the batch: 0.6875. Accuracy on the test data: 0.7358\n",
      "At epoch 2, minibatch 600. Loss: 0.6873. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7431\n",
      "At epoch 2, minibatch 1200. Loss: 0.7893. Accuracy on the batch: 0.6562. Accuracy on the test data: 0.7457\n",
      "At epoch 2, minibatch 1800. Loss: 0.8170. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7493\n",
      "Epoch 2 finished! It took: 6.6246 seconds\n",
      "At epoch 3, minibatch 0. Loss: 0.7546. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7504\n",
      "At epoch 3, minibatch 600. Loss: 0.6735. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7464\n",
      "At epoch 3, minibatch 1200. Loss: 0.5969. Accuracy on the batch: 0.8438. Accuracy on the test data: 0.7472\n",
      "At epoch 3, minibatch 1800. Loss: 1.0600. Accuracy on the batch: 0.6875. Accuracy on the test data: 0.7467\n",
      "Epoch 3 finished! It took: 6.5411 seconds\n",
      "At epoch 4, minibatch 0. Loss: 0.9054. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7439\n",
      "At epoch 4, minibatch 600. Loss: 0.5935. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7465\n",
      "At epoch 4, minibatch 1200. Loss: 0.5356. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7482\n",
      "At epoch 4, minibatch 1800. Loss: 0.6344. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7354\n",
      "Epoch 4 finished! It took: 6.5662 seconds\n",
      "At epoch 5, minibatch 0. Loss: 0.5494. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7475\n",
      "At epoch 5, minibatch 600. Loss: 0.8096. Accuracy on the batch: 0.6250. Accuracy on the test data: 0.7452\n",
      "At epoch 5, minibatch 1200. Loss: 0.8237. Accuracy on the batch: 0.6875. Accuracy on the test data: 0.7433\n",
      "At epoch 5, minibatch 1800. Loss: 0.4874. Accuracy on the batch: 0.8125. Accuracy on the test data: 0.7468\n",
      "Epoch 5 finished! It took: 6.5308 seconds\n",
      "At epoch 6, minibatch 0. Loss: 0.7111. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7452\n",
      "At epoch 6, minibatch 600. Loss: 0.9170. Accuracy on the batch: 0.6562. Accuracy on the test data: 0.7306\n",
      "At epoch 6, minibatch 1200. Loss: 1.0429. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7479\n",
      "At epoch 6, minibatch 1800. Loss: 0.7110. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7454\n",
      "Epoch 6 finished! It took: 6.4622 seconds\n",
      "At epoch 7, minibatch 0. Loss: 0.7477. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7492\n",
      "At epoch 7, minibatch 600. Loss: 0.6140. Accuracy on the batch: 0.7500. Accuracy on the test data: 0.7436\n",
      "At epoch 7, minibatch 1200. Loss: 0.2559. Accuracy on the batch: 0.9062. Accuracy on the test data: 0.7449\n",
      "At epoch 7, minibatch 1800. Loss: 0.9022. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7378\n",
      "Epoch 7 finished! It took: 6.4998 seconds\n",
      "At epoch 8, minibatch 0. Loss: 0.9104. Accuracy on the batch: 0.6562. Accuracy on the test data: 0.7401\n",
      "At epoch 8, minibatch 600. Loss: 0.8011. Accuracy on the batch: 0.7188. Accuracy on the test data: 0.7414\n",
      "At epoch 8, minibatch 1200. Loss: 0.5399. Accuracy on the batch: 0.8125. Accuracy on the test data: 0.7441\n",
      "At epoch 8, minibatch 1800. Loss: 0.7545. Accuracy on the batch: 0.6875. Accuracy on the test data: 0.7443\n",
      "Epoch 8 finished! It took: 6.3648 seconds\n",
      "At epoch 9, minibatch 0. Loss: 0.4597. Accuracy on the batch: 0.8125. Accuracy on the test data: 0.7450\n",
      "At epoch 9, minibatch 600. Loss: 1.0978. Accuracy on the batch: 0.6562. Accuracy on the test data: 0.7460\n",
      "At epoch 9, minibatch 1200. Loss: 0.3810. Accuracy on the batch: 0.8438. Accuracy on the test data: 0.7397\n",
      "At epoch 9, minibatch 1800. Loss: 0.6757. Accuracy on the batch: 0.7812. Accuracy on the test data: 0.7437\n",
      "Epoch 9 finished! It took: 6.5416 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    start = timer()\n",
    "    for batch_num , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "    \n",
    "        the_batch = Variable(minibatch_of_images.view(32, 28*28))\n",
    "        labels = Variable(minibatch_of_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = the_net(the_batch)\n",
    "        \n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        batch_accuracy = 1.0*(predicted == labels.data).sum()/labels.size(0)\n",
    "        # we want to check the accuracy with test dataset every 600 iterations.\n",
    "        if batch_num % 600 == 0:\n",
    "\n",
    "            # calculate accuracy\n",
    "            correct = 0\n",
    "            \n",
    "            # iterate through test dataset\n",
    "            for _, (images, labels) in enumerate(test_loader):\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                outputs = the_net(images)\n",
    "                # get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy =  1.0*correct / labels.size(0)\n",
    "            \n",
    "            print(\"At epoch %i, minibatch %i. Loss: %.4f. Accuracy on the batch: %.4f. Accuracy on the test data: %.4f\"\\\n",
    "                  % (epoch, batch_num, loss.data[0], batch_accuracy, accuracy))\n",
    "    end = timer()\n",
    "    print(\"Epoch %i finished! It took: %.4f seconds\" % (epoch, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Derek Doran, Dept. of CSE, Wright State University, for ATRC Summer 2018. \n",
    "\n",
    "Homepage: https://derk--.github.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
